{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with files on folder Dataset A insert on datasets/\n",
    "# with page views json insert on datasets/page_views/\n",
    "\n",
    "path_folder = 'datasets/'\n",
    "table_names = ['subscriptions', 'sessions', 'students', 'subjects', \n",
    "               'universities', 'courses', 'student_follow_subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def quote_string_(data):\n",
    "    if data:\n",
    "        return data.split('@')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Page_views\n",
    "file_name = 'page_views'\n",
    "folder_files = listdir(path_folder+file_name)\n",
    "page_views = sqlContext.read.json(path_folder+file_name+'/*.json')\n",
    "split_col = split(page_views['studentId_clientType'], '@')\n",
    "page_views = page_views.withColumn('page_views_studentsId', split_col.getItem(0))\n",
    "page_views = page_views.withColumn('page_views_ClientType', split_col.getItem(1))\n",
    "page_views = page_views.withColumn('at', concat(page_views.at.substr(0, 10), lit(' 00:00:00'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Dataframe subscriptions created\n",
      "PySpark Dataframe sessions created\n",
      "PySpark Dataframe students created\n",
      "PySpark Dataframe subjects created\n",
      "PySpark Dataframe universities created\n",
      "PySpark Dataframe courses created\n",
      "PySpark Dataframe student_follow_subject created\n"
     ]
    }
   ],
   "source": [
    "for table in table_names:\n",
    "    exec(f\"{table} = sqlContext.read.json('{path_folder}{table}.json')\")\n",
    "    exec(f\"columns_name = {table}.columns\")\n",
    "    for column in columns_name:\n",
    "        exec(f\"{table} = {table}.withColumnRenamed('{column}', '{table}_{column}')\")\n",
    "    print(f\"PySpark Dataframe {table} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the \n",
    "tmp_df = students.join(courses, students[\"students_CourseId\"] == courses[\"courses_Id\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(universities, tmp_df[\"students_UniversityId\"] == universities[\"universities_Id\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(subscriptions, tmp_df[\"students_Id\"] == subscriptions[\"subscriptions_StudentId\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(sessions, tmp_df[\"students_Id\"] == sessions[\"sessions_StudentId\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(student_follow_subject, tmp_df[\"students_Id\"] == student_follow_subject[\"student_follow_subject_StudentId\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(subjects, tmp_df[\"student_follow_subject_SubjectId\"] == subjects[\"subjects_Id\"], \"left_outer\")\n",
    "tmp_df = tmp_df.join(page_views, tmp_df[\"students_Id\"] == page_views[\"page_views_studentsId\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[at: string, courses_name: string, universities_name: string, subscriptions_PlanType: string, students_city: string, students_StudentClient: string, subjects_name: string, count: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df \\\n",
    "    .groupby('at', 'courses_name','universities_name', 'subscriptions_PlanType', 'students_city', 'students_StudentClient', 'subjects_name') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing table at bigquery, but its only possible to run in a dataproc cluster.\n",
    "tmp_df.write.format('bigquery') \\\n",
    "  .option('table', 'Dataset.page_views_analytical') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
